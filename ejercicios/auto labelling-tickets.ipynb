{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) es un modelo generativo que permite que conjuntos de observaciones puedan ser explicados por grupos no observados que explican por qué algunas partes de los datos son similares. Por ejemplo, si las observaciones son palabras en documentos, presupone que cada documento es una mezcla de un pequeño número de categorías (también denominados como tópicos) y la aparición de cada palabra en un documento se debe a una de las categorías a las que el documento pertenece. LDA es un ejemplo de modelo de categorías\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los datos de los tickets de dcip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1070, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jenkins dejado funcionar</td>\n",
       "      <td>partir horas dejado funcionar sospecha causas ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>usuario para artefactos artifactory</td>\n",
       "      <td>necesito artifact estamos generando departamen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>permisos carpetas dcip</td>\n",
       "      <td>podriais permisos carpetas eleven paths dcip 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>repo artifactory</td>\n",
       "      <td>repositorio artifactory para guardar imagenes ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               subject  \\\n",
       "0             jenkins dejado funcionar   \n",
       "1  usuario para artefactos artifactory   \n",
       "2               permisos carpetas dcip   \n",
       "3                     repo artifactory   \n",
       "\n",
       "                                             content  \n",
       "0  partir horas dejado funcionar sospecha causas ...  \n",
       "1  necesito artifact estamos generando departamen...  \n",
       "2  podriais permisos carpetas eleven paths dcip 1...  \n",
       "3  repositorio artifactory para guardar imagenes ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import IPython\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"datasets/tickets.csv\")\n",
    "\n",
    "print(df.shape)\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hacemos el preprocesado ##\n",
    " - Quitamos las stop words\n",
    " - Vectorizamos el texto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'al', 'algo', 'algunas', 'algunos', 'ante', 'antes', 'como', 'con', 'contra', 'cual', 'cuando', 'de', 'del', 'desde', 'donde', 'durante', 'e', 'el', 'ella', 'ellas', 'ellos', 'en', 'entre', 'era', 'erais', 'eran', 'eras', 'eres', 'es', 'esa', 'esas', 'ese', 'eso', 'esos', 'esta', 'estaba', 'estabais', 'estaban', 'estabas', 'estad', 'estada', 'estadas', 'estado', 'estados', 'estamos', 'estando', 'estar', 'estaremos', 'estará', 'estarán', 'estarás', 'estaré', 'estaréis', 'estaría', 'estaríais', 'estaríamos', 'estarían', 'estarías', 'estas', 'este', 'estemos', 'esto', 'estos', 'estoy', 'estuve', 'estuviera', 'estuvierais', 'estuvieran', 'estuvieras', 'estuvieron', 'estuviese', 'estuvieseis', 'estuviesen', 'estuvieses', 'estuvimos', 'estuviste', 'estuvisteis', 'estuviéramos', 'estuviésemos', 'estuvo', 'está', 'estábamos', 'estáis', 'están', 'estás', 'esté', 'estéis', 'estén', 'estés', 'fue', 'fuera', 'fuerais', 'fueran', 'fueras', 'fueron', 'fuese', 'fueseis', 'fuesen', 'fueses', 'fui', 'fuimos', 'fuiste', 'fuisteis', 'fuéramos', 'fuésemos', 'ha', 'habida', 'habidas', 'habido', 'habidos', 'habiendo', 'habremos', 'habrá', 'habrán', 'habrás', 'habré', 'habréis', 'habría', 'habríais', 'habríamos', 'habrían', 'habrías', 'habéis', 'había', 'habíais', 'habíamos', 'habían', 'habías', 'han', 'has', 'hasta', 'hay', 'haya', 'hayamos', 'hayan', 'hayas', 'hayáis', 'he', 'hemos', 'hube', 'hubiera', 'hubierais', 'hubieran', 'hubieras', 'hubieron', 'hubiese', 'hubieseis', 'hubiesen', 'hubieses', 'hubimos', 'hubiste', 'hubisteis', 'hubiéramos', 'hubiésemos', 'hubo', 'la', 'las', 'le', 'les', 'lo', 'los', 'me', 'mi', 'mis', 'mucho', 'muchos', 'muy', 'más', 'mí', 'mía', 'mías', 'mío', 'míos', 'nada', 'ni', 'no', 'nos', 'nosotras', 'nosotros', 'nuestra', 'nuestras', 'nuestro', 'nuestros', 'o', 'os', 'otra', 'otras', 'otro', 'otros', 'para', 'pero', 'poco', 'por', 'porque', 'que', 'quien', 'quienes', 'qué', 'se', 'sea', 'seamos', 'sean', 'seas', 'seremos', 'será', 'serán', 'serás', 'seré', 'seréis', 'sería', 'seríais', 'seríamos', 'serían', 'serías', 'seáis', 'sido', 'siendo', 'sin', 'sobre', 'sois', 'somos', 'son', 'soy', 'su', 'sus', 'suya', 'suyas', 'suyo', 'suyos', 'sí', 'también', 'tanto', 'te', 'tendremos', 'tendrá', 'tendrán', 'tendrás', 'tendré', 'tendréis', 'tendría', 'tendríais', 'tendríamos', 'tendrían', 'tendrías', 'tened', 'tenemos', 'tenga', 'tengamos', 'tengan', 'tengas', 'tengo', 'tengáis', 'tenida', 'tenidas', 'tenido', 'tenidos', 'teniendo', 'tenéis', 'tenía', 'teníais', 'teníamos', 'tenían', 'tenías', 'ti', 'tiene', 'tienen', 'tienes', 'todo', 'todos', 'tu', 'tus', 'tuve', 'tuviera', 'tuvierais', 'tuvieran', 'tuvieras', 'tuvieron', 'tuviese', 'tuvieseis', 'tuviesen', 'tuvieses', 'tuvimos', 'tuviste', 'tuvisteis', 'tuviéramos', 'tuviésemos', 'tuvo', 'tuya', 'tuyas', 'tuyo', 'tuyos', 'tú', 'un', 'una', 'uno', 'unos', 'vosotras', 'vosotros', 'vuestra', 'vuestras', 'vuestro', 'vuestros', 'y', 'ya', 'yo', 'él', 'éramos']\n"
     ]
    }
   ],
   "source": [
    "from stop_words import get_stop_words\n",
    "stop  = get_stop_words('spanish')\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>partir horas dejado funcionar sospecha causas ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>necesito artifact estamos generando departamen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>podriais permisos carpetas eleven paths dcip 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>repositorio artifactory para guardar imagenes ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              ticket\n",
       "0  partir horas dejado funcionar sospecha causas ...\n",
       "1  necesito artifact estamos generando departamen...\n",
       "2  podriais permisos carpetas eleven paths dcip 1...\n",
       "3  repositorio artifactory para guardar imagenes ..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "NUM_TICKETS=1000\n",
    "# Cargamos DF\n",
    "df = pd.read_csv(\"datasets/tickets.csv\")\n",
    "df = df.head(NUM_TICKETS)\n",
    "# Quitamos los campos vacios\n",
    "df['subject'].fillna('', inplace=True)\n",
    "df['content'].fillna('', inplace=True)\n",
    "# Un solo campo concatenando Subject + Content\n",
    "df[\"ticket\"] = df['content'] + df['subject']\n",
    "# Borrar columnas\n",
    "del df['content'] \n",
    "del df['subject']\n",
    "\n",
    "df.head(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.1, max_features=76, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None,\n",
      "        stop_words=['a', 'al', 'algo', 'algunas', 'algunos', 'ante', 'antes', 'como', 'con', 'contra', 'cual', 'cuando', 'de', 'del', 'desde', 'donde', 'durante', 'e', 'el', 'ella', 'ellas', 'ellos', 'en', 'entre', 'era', 'erais', 'eran', 'eras', 'eres', 'es', 'esa', 'esas', 'ese', 'eso', 'esos', 'esta', 'e...osotras', 'vosotros', 'vuestra', 'vuestras', 'vuestro', 'vuestros', 'y', 'ya', 'yo', 'él', 'éramos'],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 2 0]\n",
      " [0 0 0 ... 0 0 0]] (1000, 76) (1, 76)\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer implements both tokenization and occurrence counting\n",
    "#  in a single class:\n",
    "MAX_FEATURES=76\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer( max_df=.1,max_features=MAX_FEATURES,stop_words=stop,ngram_range=(1, 3))\n",
    "X = count.fit_transform(df['ticket'].values)\n",
    "print(count)\n",
    "print(X.toarray()  , X.shape , X[0].shape  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acceder', 'access', 'amd64', 'analytics', 'android', 'attached', 'aubay', 'ayuda', 'built', 'cligitapiimpl', 'command', 'configuracion', 'contacta', 'contint', 'contraseña', 'crear', 'david', 'delivery', 'direccion', 'documento', 'ejecucion', 'entorno', 'esclavo', 'export', 'hace', 'help', 'hotfix', 'hotfix built', 'hudson', 'imagen', 'iplanet', 'iplanet messaging', 'iplanet messaging server', 'ituser', 'java', 'java hudson', 'jenkins dcip', 'jenkinsci', 'jira', 'marcossuarez', 'messaging', 'messaging server', 'messaging server hotfix', 'necesito', 'network', 'openstack', 'origin', 'parece', 'plugin', 'plugins', 'problem', 'prodepgdcipm01', 'pruebas', 'pull', 'puppet', 'received', 'release', 'remote', 'repositorio', 'returned', 'script', 'server hotfix', 'server hotfix built', 'setting', 'slave', 'smtp', 'soporte', 'space', 'system', 'tefdigital', 'tener', 'test', 'utf8', 'view', 'wikis', 'workspace']\n"
     ]
    }
   ],
   "source": [
    "print(count.get_feature_names())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sacamos los tópicos de cada ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_topics=5,\n",
    "                                random_state=123,\n",
    "                                learning_method='batch')\n",
    "X_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "received messaging prodepgdcipm01 smtp analytics hotfix\n",
      "Topic 2:\n",
      "esclavo access imagen aubay documento slave\n",
      "Topic 3:\n",
      "plugin script contint repositorio david crear\n",
      "Topic 4:\n",
      "ituser openstack jira system attached help\n",
      "Topic 5:\n",
      "java hudson plugins jenkinsci java hudson amd64\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 6\n",
    "feature_names = count.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Topic %d:\" % (topic_idx + 1))\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()\\\n",
    "                        [:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caso [223 224 220 221 222 429 501 502 508 509 415 187 150 702 105 722   6 552\n",
      " 728 397 458 445  92 868 662  15 197 611  93 792 168 731 776 178 971 844\n",
      "  23 633 930 442 327 157 478 100 164 840 616 780 725 680  11 305 755 369\n",
      " 864 691 614 171 243 645 855 856  58 414 807 587 719 720 698 638 697 639\n",
      "  89 283  76 906 337 405 929 843 230 232 231 706 121 845 285 229 115 114\n",
      " 113 287 228 817 108 107 227 102 469 111 718 122 263 730 139 140 141 143\n",
      " 144 145 268 147 148 262 815 260 256 255 526 160 166 528 540 172 174 137\n",
      " 542 136 784 125 233 127 481 713 128 714 715 129 716 130 132 717 796 544\n",
      " 133 271 134 135 786 727 288 126 696 649 366 367 368 371 955 372  21 374\n",
      " 375 376 640 380 381 314  24 383 941 641 612 311 307 933 182  14 364 964\n",
      " 981 997 995 993 343   2 210   4 346 983 348 349 363 978 350 351 352 636\n",
      " 973 972 637 354 362 213 605 591 446 300 432 299 217 298  68 883 882 297\n",
      "  77  80 579  81 870 866  86 863 690  90 692 854 225 214 678 422 673 215\n",
      " 588 191 659 304 669 671 421 585 301 175  49 674 216 303 302 138  50 777\n",
      " 875 430 623   8 176 610 570 804 338 244 974 325 665 586 545  10 908 403\n",
      " 402 418 417 737 398 401 400 393 937 419  75 340 618 628 809 824 392 848\n",
      "  99 681 482 189 738 679 885 424 423 289  91 183 384 470 286 994 726 646\n",
      " 185 666  55  53 550 924 921 805 109 920 903 464 479 270 272 190  33 277\n",
      " 269 317 319 524 276 480 292 280 451 278 436 450 437 282 439 330 664 812\n",
      " 778 407 334 990 594 818 245 867 795 543 592 593 527 385 785 783 711 734\n",
      " 541 116 851 186 273 707  72  30 209  42 650 723   0 159 655 146 957 982\n",
      "  17 757 607 861 117 905 946 802 790 199 574  52 596 394 318 754 206 682\n",
      " 688 820 663 257 797 799 595 654 721 370 794 547 729 386 409 675  20 388\n",
      " 705 950 656 644 642 630 735 440 703  39 261  27 254 252 512 154 511 522\n",
      " 404 510 525 167  31 248 242 238 259 151  69 264 181 434 200 205  79 447\n",
      " 433 449  83   5 212 294 291 290 258 279 267  16 274 201 281 275 236 980\n",
      "  12 892 927  36 365 104  48 712  18 947 563  40 791  13  35 945  32 389\n",
      " 709 321 677 948 188 577 323 689 218 615 465 798 583 453 951 934 557 916\n",
      " 787 250 629 902   3 602 975 890 561 825 361  97 356 467 555 234  41 756\n",
      " 454  46 204 153 601 192  47 942 901 553 158 899 295 599 823 925 572 699\n",
      " 461 686 998 632  65 101 580 657 342 954 736  34 781 620  78 562 193 598\n",
      " 810 626 523  28 858 661 546 241 521 379 426 600 648 571 358 584 970 306\n",
      " 249 448 708  95 806  94 387 551 827 473  54 529 857 710 180   7 700 390\n",
      " 643 420 996 752  66 165 922 173   9 672 953 684 322 484 456 976 938 170\n",
      " 935  71 944 477 635 208  73 578 408 732 156  45 184 569 247 693 597 647\n",
      " 897 992 391 120  26 195 413 355 411 443 822 410 969 683  57 606 581  62\n",
      " 609 332 326 444  29 603 753 416 967 462 179 459   1 112 999 399 576 668\n",
      "  56 949 943  37 939  22 119 123 455 739 382 865 952 341 142 558 427 751\n",
      " 631 373 685 821 979 850 653 251 694 284 549  87 940 333  85 194 651  38\n",
      " 788 932 670 658 895 936 207 701 328 859 676 741 852 793  43  44 613 813\n",
      " 198 313 816 316 660 378 554 428 452 803 742 896 353 412 163 977 152 466\n",
      " 907 310 826 329 483 253  64 984 808 559 169 345 819 471 457 315 604 733\n",
      " 312  63 396 860 344 131 853 966 347 296 740 211 956 590  98 828 801 438\n",
      " 435 196 968 360 336 335 617 476 667 320 460 103 564 695 517 518 862 556\n",
      " 779 530 324 463 747 589 652 766 764 775 774 765 761 608 963 468 724 847\n",
      " 575 425 582 987 266  67 842  88 989 986 988 568 634 782 331 744  19 770\n",
      " 769 773 771 772 762 872 873 874 898 991 884 118 800 746 745 900 838 835\n",
      " 830 831 832 833 834 836 837 839 841 789 846 627 619  70 759 760 763 768\n",
      " 767 931 560  51 106  74 308 912 441 849 359 395 923  84 829 814 893 339\n",
      " 406 202 869 961 960 915 871  25  96 625 743 515 520 886 894 926 687 811\n",
      " 959 962 965 513 519 514 516 624 621 622 891 704 573  82 985 750 309 155\n",
      " 110 914 911 913 904 749 431 909 910 293 879 876 888 889 880 881 919 877\n",
      " 887 878 539 475 758 928 149 226 917 918 124 474 748 507  61 239 161 162\n",
      "  59  60 240 177 357 237 235 538 536 246 472 537 535 497 566 565 486 567\n",
      " 219 377 485 487 496 534 498 265 500 495 494 503 493 492 504 499 505 506\n",
      " 531 532 533 491 490 489 488 203 958 548]\n",
      "\n",
      "Topic #1:\n",
      "system host tefdigital sorry inform could delivered more recipients attached further assistance postmaster include problem report delete text attached returned system connect connection timed separator dcip analytics vasvalue release redirect started change envinject loading node environment variabl ...\n",
      "\n",
      "Topic #2:\n",
      "system host tefdigital sorry inform could delivered more recipients attached further assistance postmaster include problem report delete text attached returned system fjem connect connection timed separator dcip analytics acquisition release spark redirect started change envinject loading node envir ...\n",
      "\n",
      "Topic #3:\n",
      "system host tefdigital sorry inform could delivered more recipients attached further assistance postmaster include problem report delete text attached returned system connect connection timed separator dcip analytics vasvalue release redirect started change envinject loading node environment variabl ...\n",
      "\n",
      "Topic #4:\n",
      "system host tefdigital sorry inform could delivered more recipients attached further assistance postmaster include problem report delete text attached returned system connect connection timed separator dcip analytics vasvalue release redirect started change envinject loading node environment variabl ...\n",
      "\n",
      "Topic #5:\n",
      "system host tefdigital sorry inform could delivered more recipients attached further assistance postmaster include problem report delete text attached returned system connect connection timed separator dcip analytics vasvalue release redirect started change envinject loading node environment variabl ...\n",
      "\n",
      "Topic #6:\n",
      "estoy ejecutando dcip analytics neptuno neptuno hace rato esta saliendo error parece muchos descriptores fichero abierto permisos jenkins podeis echar mano muchas antemano rebeca started usercx01873 dcip cx01873 revparse isinsideworktree timeout error workspace repository corrupt hudson plugins gite ...\n",
      "\n",
      "Topic #7:\n",
      "system host tefdigital sorry inform could delivered more recipients attached further assistance postmaster include problem report delete text attached returned system henar munozfrutos host said must issue starttls command first command separator dcip analytics smart energy smart enery build truncat ...\n",
      "\n",
      "Topic #8:\n",
      "system host tefdigital sorry inform could delivered more recipients attached further assistance postmaster include problem report delete text attached returned system henar munozfrutos host said must issue starttls command first command separator dcip analytics smart energy smart enery build truncat ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mytopic = X_topics[:, 4].argsort()[::-1]\n",
    "print (\"Caso\", mytopic)\n",
    "for iter_idx, movie_idx in enumerate(mytopic[:8]):\n",
    "    print('\\nTopic #%d:' % (iter_idx + 1))\n",
    "    print(df['ticket'][movie_idx][:300], '...')\n",
    "    #print(df['content'][movie_idx][:300], '...')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-a6022fe46567>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mn_top_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdata_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/envs/py36/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3612\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3613\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3614\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3616\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'content'"
     ]
    }
   ],
   "source": [
    "# Use tf-idf features for NMF.\n",
    "\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "n_samples = 5000\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "\n",
    "data_samples = df.content[:n_samples]\n",
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(df['content'])\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
    "      \"tf-idf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
